{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm /opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info -rdf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:41:26.949474Z","iopub.execute_input":"2024-11-11T00:41:26.949809Z","iopub.status.idle":"2024-11-11T00:41:27.982988Z","shell.execute_reply.started":"2024-11-11T00:41:26.949769Z","shell.execute_reply":"2024-11-11T00:41:27.981684Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:41:27.984917Z","iopub.execute_input":"2024-11-11T00:41:27.985268Z","iopub.status.idle":"2024-11-11T00:41:42.986664Z","shell.execute_reply.started":"2024-11-11T00:41:27.985233Z","shell.execute_reply":"2024-11-11T00:41:42.985745Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=427be222e1a7d4ee5858d28932fda73543977dc5f3af0cd855c5bdaffeee5838\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install evaluate ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:41:42.989053Z","iopub.execute_input":"2024-11-11T00:41:42.989479Z","iopub.status.idle":"2024-11-11T00:41:54.933890Z","shell.execute_reply.started":"2024-11-11T00:41:42.989431Z","shell.execute_reply":"2024-11-11T00:41:54.932767Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nimport nltk\n\nimport transformers\nfrom datasets import load_dataset\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:41:54.935411Z","iopub.execute_input":"2024-11-11T00:41:54.935764Z","iopub.status.idle":"2024-11-11T00:42:14.549024Z","shell.execute_reply.started":"2024-11-11T00:41:54.935727Z","shell.execute_reply":"2024-11-11T00:42:14.548246Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:14.551475Z","iopub.execute_input":"2024-11-11T00:42:14.552238Z","iopub.status.idle":"2024-11-11T00:42:14.590583Z","shell.execute_reply.started":"2024-11-11T00:42:14.552193Z","shell.execute_reply":"2024-11-11T00:42:14.589717Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"billsum = load_dataset('billsum', split='ca_test')\nbillsum = billsum.train_test_split(test_size=.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:14.591737Z","iopub.execute_input":"2024-11-11T00:42:14.592031Z","iopub.status.idle":"2024-11-11T00:42:21.843431Z","shell.execute_reply.started":"2024-11-11T00:42:14.592000Z","shell.execute_reply":"2024-11-11T00:42:21.842515Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397b325c7525418b8d81e21c1eb93f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/91.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c44f3a367628443d9974509066e3657d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/15.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb356161b478449aa7b470f18fca529d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ca_test-00000-of-00001.parquet:   0%|          | 0.00/6.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c86a8c1c1281447ab8ccfd99815e18ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/18949 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2073cc0c574e4bb647a098dc7896b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3269 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d3f53db518c4412a99e5bc999e5e7c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating ca_test split:   0%|          | 0/1237 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"457a462a8eca476790577bab909df766"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained('ainize/bart-base-cnn')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:21.844765Z","iopub.execute_input":"2024-11-11T00:42:21.845463Z","iopub.status.idle":"2024-11-11T00:42:23.475520Z","shell.execute_reply.started":"2024-11-11T00:42:21.845417Z","shell.execute_reply":"2024-11-11T00:42:23.474497Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24c7ef9076f448b5b0e7410c3d3c0e85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b1e36d83204388b2cbd5ac6b7d4363"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd4cccd95fd145eabdaac75e5b58c009"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e668f7dced54402da68836a2e68dfe8e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples['text'], max_length=1024, truncation=True\n    )\n    labels = tokenizer(\n        text_target=examples['summary'], max_length=128, truncation=True\n    )\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:23.476952Z","iopub.execute_input":"2024-11-11T00:42:23.477349Z","iopub.status.idle":"2024-11-11T00:42:23.483041Z","shell.execute_reply.started":"2024-11-11T00:42:23.477304Z","shell.execute_reply":"2024-11-11T00:42:23.482103Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenized_billsum = billsum.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:23.484188Z","iopub.execute_input":"2024-11-11T00:42:23.484468Z","iopub.status.idle":"2024-11-11T00:42:28.543929Z","shell.execute_reply.started":"2024-11-11T00:42:23.484437Z","shell.execute_reply":"2024-11-11T00:42:28.542780Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f07e03e8cd344d98f66cd8ce3a6cf86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/124 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d9e4eb5806431ca802004abd546876"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"nltk.download('punkt', quiet=True)\nmetric = evaluate.load('rouge')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:28.545296Z","iopub.execute_input":"2024-11-11T00:42:28.545668Z","iopub.status.idle":"2024-11-11T00:42:29.267668Z","shell.execute_reply.started":"2024-11-11T00:42:28.545633Z","shell.execute_reply":"2024-11-11T00:42:29.266756Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7a800385ad4acb95d4c29fc981c017"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # decode preds and labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # rougeLSum expects newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:29.271149Z","iopub.execute_input":"2024-11-11T00:42:29.271464Z","iopub.status.idle":"2024-11-11T00:42:29.278100Z","shell.execute_reply.started":"2024-11-11T00:42:29.271431Z","shell.execute_reply":"2024-11-11T00:42:29.277198Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model = transformers.AutoModelForSeq2SeqLM.from_pretrained('ainize/bart-base-cnn')\n# Batching function\ndata_collator = transformers.DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:29.279765Z","iopub.execute_input":"2024-11-11T00:42:29.280119Z","iopub.status.idle":"2024-11-11T00:42:34.078581Z","shell.execute_reply.started":"2024-11-11T00:42:29.280078Z","shell.execute_reply":"2024-11-11T00:42:34.077747Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5634b559c8a644ca8a16b828b2c2d2e5"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # decode preds and labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # rougeLSum expects newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:42:34.080099Z","iopub.execute_input":"2024-11-11T00:42:34.080482Z","iopub.status.idle":"2024-11-11T00:42:34.087378Z","shell.execute_reply.started":"2024-11-11T00:42:34.080438Z","shell.execute_reply":"2024-11-11T00:42:34.086263Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Define a function to compute ROUGE scores\ndef compute_metrics(eval_preds):\n    predictions, labels = eval_preds\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Initialize ROUGE scorer\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n\n    # Calculate ROUGE for each prediction\n    for ref, pred in zip(decoded_labels, decoded_preds):\n        scores = scorer.score(ref, pred)\n        rouge_scores[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n        rouge_scores[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n        rouge_scores[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n\n    # Calculate average ROUGE scores\n    avg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\n    avg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\n    avg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n\n    return {\n        \"rouge1\": avg_rouge1,\n        \"rouge2\": avg_rouge2,\n        \"rougeL\": avg_rougeL,\n    }\n\n# Define Seq2SeqTrainingArguments\ntraining_args = transformers.Seq2SeqTrainingArguments(\n    output_dir='./bart_finetuning_results',\n    evaluation_strategy='steps',\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=2,\n    fp16=True,\n    predict_with_generate=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"steps\",  # Specify metric for model selection\n    greater_is_better=True,  # ROUGE-L's F1 is higher when better\n    report_to=[\"none\"]  # Disable external logging like WandB\n)\n\n# Initialize the Seq2SeqTrainer\ntrainer = transformers.Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_billsum['train'],\n    eval_dataset=tokenized_billsum['test'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,  # Pass the compute_metrics function\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:37:38.217240Z","iopub.execute_input":"2024-11-11T02:37:38.218063Z","iopub.status.idle":"2024-11-11T02:37:38.262652Z","shell.execute_reply.started":"2024-11-11T02:37:38.218021Z","shell.execute_reply":"2024-11-11T02:37:38.261252Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:37:40.979211Z","iopub.execute_input":"2024-11-11T02:37:40.979601Z","iopub.status.idle":"2024-11-11T02:40:46.264532Z","shell.execute_reply.started":"2024-11-11T02:37:40.979561Z","shell.execute_reply":"2024-11-11T02:40:46.263606Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 03:01, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=70, training_loss=0.7242133549281529, metrics={'train_runtime': 184.5585, 'train_samples_per_second': 12.061, 'train_steps_per_second': 0.379, 'total_flos': 1357273356042240.0, 'train_loss': 0.7242133549281529, 'epoch': 2.0})"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"#text_example=billsum['test']['text'][42]\ntext_example='''\n\nBorn in New York City, Trump graduated with a bachelor's degree in economics from the University of Pennsylvania in 1968. After becoming president of the family real estate business in 1971, Trump renamed it the Trump Organization and reoriented the company toward building and renovating skyscrapers, hotels, casinos, and golf courses. After a series of business failures in the late 1990s, he launched side ventures, mostly licensing the Trump name. From 2004 to 2015, he produced and hosted the reality television series The Apprentice. He and his businesses have been involved in more than 4,000 legal actions, including six business bankruptcies.\n\nTrump won the 2016 presidential election as the Republican Party nominee, defeating the Democratic Party candidate, Hillary Clinton, while losing the popular vote,[a] and became the first U.S. president without prior military or government service. The Mueller investigation later determined that Russia interfered in the 2016 election to help Trump. His campaign positions were described as populist, protectionist, and nationalist. His election and policies sparked numerous protests and led to the creation of Trumpism: a political movement and a cult of personality.[b] Trump promoted conspiracy theories and made many false and misleading statements during his campaigns and presidency, to a degree unprecedented in American politics. Many of his comments and actions have been characterized as racially charged, racist, and misogynistic.\n\nIn his first term, Trump ordered a travel ban on citizens from several Muslim-majority countries, diverted military funding toward building a wall on the U.S.–Mexico border, and implemented a family separation policy. He rolled back more than 100 environmental policies and regulations and signed the Tax Cuts and Jobs Act of 2017, which cut taxes and eliminated the individual mandate penalty of the Affordable Care Act. He appointed Neil Gorsuch, Brett Kavanaugh, and Amy Coney Barrett to the U.S. Supreme Court. He reacted slowly to the COVID-19 pandemic, ignored or contradicted recommendations from health officials, used political pressure to interfere with testing efforts, and spread unverified information about unproven treatments. Trump initiated a trade war with China and withdrew the U.S. from the proposed Trans-Pacific Partnership trade agreement, the Paris Agreement on climate change, and the Iran nuclear deal. He met with North Korean leader Kim Jong Un three times, but made no progress on denuclearization. After his first term, scholars and historians ranked Trump one of the worst presidents in American history.\n\nTrump lost the 2020 presidential election to Democrat Joe Biden but refused to concede, falsely claiming widespread electoral fraud and attempting to overturn the results. On January 6, 2021, Trump urged his supporters to march to the U.S. Capitol, which many of them attacked. He is the only U.S. president to have been impeached twice: his first impeachment in 2019 for abuse of power and obstruction of Congress after he pressured Ukraine to investigate Biden, and his second impeachment in 2021 for incitement of insurrection; the Senate acquitted him in both cases. In 2024, Donald trump was prosecuted in New York wherein the jury found him guilty of falsifying business records related to his hush money payment to porn star Stormy Daniels, making him the first U.S. president to be convicted of a felony. Trump faced more felony indictments related to his alleged mishandling of classified documents and interference in the 2020 election, and he was found liable in civil trials for the sexual abuse and defamation of E. Jean Carroll, and the financial fraud by the Trump Organization.'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:31:17.782542Z","iopub.execute_input":"2024-11-11T02:31:17.783443Z","iopub.status.idle":"2024-11-11T02:31:17.790911Z","shell.execute_reply.started":"2024-11-11T02:31:17.783400Z","shell.execute_reply":"2024-11-11T02:31:17.789982Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"input_ids = tokenizer.encode(\n    text_example,\n    return_tensors=\"pt\",\n    max_length=1024,\n    truncation=True,\n).to(device)\ninput_ids.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:31:19.028543Z","iopub.execute_input":"2024-11-11T02:31:19.029185Z","iopub.status.idle":"2024-11-11T02:31:19.039571Z","shell.execute_reply.started":"2024-11-11T02:31:19.029146Z","shell.execute_reply":"2024-11-11T02:31:19.038658Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 709])"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"summary_text_ids = model.generate(\n    input_ids=input_ids,\n    bos_token_id=model.config.bos_token_id,\n    eos_token_id=model.config.eos_token_id,\n    max_length=50,\n    min_length=56,\n    num_beams=4,\n)\ndecoded_text = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)\nprint(decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:31:42.210804Z","iopub.execute_input":"2024-11-11T02:31:42.211185Z","iopub.status.idle":"2024-11-11T02:31:42.829286Z","shell.execute_reply.started":"2024-11-11T02:31:42.211148Z","shell.execute_reply":"2024-11-11T02:31:42.828351Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1244: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"The Trump Organization is a business empire headed by billionaire real estate developer Donald Trump, who is also known as the President of the Trump Organization. The business has been involved in more than 4,000 legal actions, including 6 business bankruptcies\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"trainer.save_model('./kaggle/working/')\n\ntokenizer.save_pretrained('./kaggle/working/')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T10:10:22.115804Z","iopub.execute_input":"2024-11-08T10:10:22.116211Z","iopub.status.idle":"2024-11-08T10:10:23.407011Z","shell.execute_reply.started":"2024-11-08T10:10:22.116149Z","shell.execute_reply":"2024-11-08T10:10:23.406110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install huggingface_hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T01:24:06.441382Z","iopub.execute_input":"2024-11-11T01:24:06.441807Z","iopub.status.idle":"2024-11-11T01:24:17.884046Z","shell.execute_reply.started":"2024-11-11T01:24:06.441767Z","shell.execute_reply":"2024-11-11T01:24:17.882926Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"!\n\nhuggingface-cli login\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T01:24:26.401498Z","iopub.execute_input":"2024-11-11T01:24:26.401906Z","iopub.status.idle":"2024-11-11T01:28:19.392612Z","shell.execute_reply.started":"2024-11-11T01:24:26.401867Z","shell.execute_reply":"2024-11-11T01:28:19.391399Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\nEnter your token (input will not be visible): Traceback (most recent call last):\n  File \"/opt/conda/bin/huggingface-cli\", line 8, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n    service.run()\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 115, in login\n    interpreter_login(new_session=new_session, write_permission=write_permission)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n    token = getpass(\"Enter your token (input will not be visible): \")\n  File \"/opt/conda/lib/python3.10/getpass.py\", line 77, in unix_getpass\n    passwd = _raw_input(prompt, stream, input=input)\n  File \"/opt/conda/lib/python3.10/getpass.py\", line 146, in _raw_input\n    line = input.readline()\n  File \"/opt/conda/lib/python3.10/codecs.py\", line 319, in decode\n    def decode(self, input, final=False):\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import os\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T01:28:25.940242Z","iopub.execute_input":"2024-11-11T01:28:25.940619Z","iopub.status.idle":"2024-11-11T01:28:25.944919Z","shell.execute_reply.started":"2024-11-11T01:28:25.940582Z","shell.execute_reply":"2024-11-11T01:28:25.943972Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=os.getenv(\"HUGGINGFACE_HUB_TOKEN\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T01:28:29.400940Z","iopub.execute_input":"2024-11-11T01:28:29.401288Z","iopub.status.idle":"2024-11-11T01:28:29.537603Z","shell.execute_reply.started":"2024-11-11T01:28:29.401254Z","shell.execute_reply":"2024-11-11T01:28:29.536747Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!huggingface-cli repo create <First_Model> --type=model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T01:29:34.859637Z","iopub.execute_input":"2024-11-11T01:29:34.860451Z","iopub.status.idle":"2024-11-11T01:29:35.884846Z","shell.execute_reply.started":"2024-11-11T01:29:34.860406Z","shell.execute_reply":"2024-11-11T01:29:35.883777Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"/bin/bash: line 1: First_Model: No such file or directory\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"model.save_pretrained(\"./First_Model1\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:42:55.963225Z","iopub.execute_input":"2024-11-11T02:42:55.963634Z","iopub.status.idle":"2024-11-11T02:42:56.897656Z","shell.execute_reply.started":"2024-11-11T02:42:55.963594Z","shell.execute_reply":"2024-11-11T02:42:56.896867Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"!mkdir /kaggle/working/New_Model_Directory\n!cd /kaggle/working/New_Model_Directory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:49:52.409093Z","iopub.execute_input":"2024-11-11T02:49:52.409848Z","iopub.status.idle":"2024-11-11T02:49:54.490056Z","shell.execute_reply.started":"2024-11-11T02:49:52.409807Z","shell.execute_reply":"2024-11-11T02:49:54.488584Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"!https://huggingface.co/<Advik-7>/<First_Model>\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T01:38:49.232373Z","iopub.execute_input":"2024-11-11T01:38:49.233395Z","iopub.status.idle":"2024-11-11T01:38:50.264673Z","shell.execute_reply.started":"2024-11-11T01:38:49.233340Z","shell.execute_reply":"2024-11-11T01:38:50.263527Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n/bin/bash: -c: line 1: `https://huggingface.co/<Advik-7>/<First_Model>'\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"from huggingface_hub import upload_folder\n\n# Replace 'path_to_your_folder' with the path to the folder containing your model and tokenizer files\nfolder_path = \"/kaggle/working/bart_finetuning_results/checkpoint-210\"\nrepo_id = \"Advik-7/First_Model\"  # Replace with your Hugging Face repository ID\ntoken = \"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\"  # Replace with your actual Hugging Face token\n\nupload_folder(\n    folder_path=folder_path,\n    repo_id=repo_id,\n    repo_type=\"model\",\n    token=token,\n    ignore_patterns=[\"*.DS_Store\"],  # Adjust if necessary\n    commit_message=\"Upload model and tokenizer files\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T01:47:35.929963Z","iopub.execute_input":"2024-11-11T01:47:35.930920Z","iopub.status.idle":"2024-11-11T01:48:13.482113Z","shell.execute_reply.started":"2024-11-11T01:47:35.930875Z","shell.execute_reply":"2024-11-11T01:48:13.481114Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb2d263b6e64e269cfdd59ea500fb47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102e3e8f011944ccba431a76f40e3598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96706a6ae51f4d11b80f847406a3225b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5422bc5a8aa54b5c962af891789deb15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818bbab40a734b509c637686a9172898"}},"metadata":{}},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Advik-7/First_Model/commit/3e3e028695bef788a39aef20164dc09b190ca194', commit_message='Upload model and tokenizer files', commit_description='', oid='3e3e028695bef788a39aef20164dc09b190ca194', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Advik-7/First_Model', endpoint='https://huggingface.co', repo_type='model', repo_id='Advik-7/First_Model'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"from transformers import AutoTokenizer, BartForConditionalGeneration\n\n# Replace with your model's Hugging Face repository name\nrepo_name = \"Advik-7/First_Model\"\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(repo_name)\n\n# Load the model (use BartForConditionalGeneration for summarization)\nmodel = BartForConditionalGeneration.from_pretrained(repo_name)\n\n# Check if everything is loaded correctly\nprint(model)\nprint(tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T01:52:57.964794Z","iopub.execute_input":"2024-11-11T01:52:57.965531Z","iopub.status.idle":"2024-11-11T01:52:59.289681Z","shell.execute_reply.started":"2024-11-11T01:52:57.965492Z","shell.execute_reply":"2024-11-11T01:52:59.288784Z"}},"outputs":[{"name":"stdout","text":"BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n)\nBartTokenizerFast(name_or_path='Advik-7/First_Model', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n}\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# Sample text for summarization\nsample_text = \"\"\"The company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers.[2] The company was named after the U+1F917 🤗 HUGGING FACE emoji.[2] After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform for machine learning.\n\nIn March 2021, Hugging Face raised US$40 million in a Series B funding round.[3]\n\nOn April 28, 2021, the company launched the BigScience Research Workshop in collaboration with several other research groups to release an open large language model.[4] In 2022, the workshop concluded with the announcement of BLOOM, a multilingual large language model with 176 billion parameters.[5][6]\n\nIn December 2022, the company acquired Gradio, an open source library built for developing machine learning applications in Python.[7]\n\nOn May 5, 2022, the company announced its Series C funding round led by Coatue and Sequoia.[8] The company received a $2 billion valuation.\n\nOn August 3, 2022, the company announced the Private Hub, an enterprise version of its public Hugging Face Hub that supports SaaS or on-premises deployment.[9]\n\nIn February 2023, the company announced partnership with Amazon Web Services (AWS) which would allow Hugging Face's products available to AWS customers to use them as the building blocks for their custom applications. The company also said the next generation of BLOOM will be run on Trainium, a proprietary machine learning chip created by AWS.[10][11][12]\n\nIn August 2023, the company announced that it raised $235 million in a Series D funding, at a $4.5 billion valuation. The funding was led by Salesforce, and notable participation came from Google, Amazon, Nvidia, AMD, Intel, IBM, and Qualcomm.[13]\n\nIn June 2024, the company announced, along with Meta and Scaleway, their launch of a new AI accelerator program for European startups. This initiative aims to help startups integrate open foundation models into their products, accelerating the EU AI ecosystem. The program, based at STATION F in Paris, will run from September 2024 to February 2025. Selected startups will receive mentoring, access to AI models and tools, and Scaleway’s computing power.[14]\n\nOn September 23, 2024, to further the International Decade of Indigenous Languages, Hugging Face teamed up with Meta and UNESCO to launch a new online language translator [15] built on Meta's No Language Left Behind open-source AI model, enabling free text translation across 200 languages, including many low-resource languages.[16]\"\"\"\n\n# Tokenize the input text\ninputs = tokenizer(sample_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n\n# Generate the summary (using the model)\nsummary_ids = model.generate(inputs['input_ids'], max_length=50, num_beams=4, early_stopping=True)\n\n# Decode the generated summary\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nprint(\"Summary:\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:05:23.988400Z","iopub.execute_input":"2024-11-11T02:05:23.989259Z","iopub.status.idle":"2024-11-11T02:05:24.256737Z","shell.execute_reply.started":"2024-11-11T02:05:23.989215Z","shell.execute_reply":"2024-11-11T02:05:24.255301Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[61], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(sample_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Generate the summary (using the model)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Decode the generated summary\u001b[39;00m\n\u001b[1;32m     29\u001b[0m summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1865\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1865\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:512\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    510\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    511\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 512\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1060\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m embed_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_positions(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1063\u001b[0m embed_pos \u001b[38;5;241m=\u001b[39m embed_pos\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:129\u001b[0m, in \u001b[0;36mBartScaledWordEmbedding.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_scale\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","output_type":"error"}],"execution_count":61},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}