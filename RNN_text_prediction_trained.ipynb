{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvhZBf0M5KkVNhNhKsXaE2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/advik-7/NLP_projects/blob/main/RNN_text_prediction_trained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U5vARGTvVZk",
        "outputId": "58cd075c-86fc-4a15-98d8-ab1722a0749f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "\n",
        "# Open the .docx file\n",
        "doc = Document('/content/moby_dick_ch4.docx')\n",
        "\n",
        "# Read the content\n",
        "for paragraph in doc.paragraphs:\n",
        "    print(paragraph.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErzgryApvaxP",
        "outputId": "cc2159e0-22fd-4cf7-f0ca-c1aa584c4a6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upon waking next morning about daylight, I found Queequeg's arm thrown over me in the most loving and affectionate manner. You had almost thought I had been his wife. The counterpane was of patchwork, full of odd little parti-coloured squares and triangles; and this arm of his tattooed all over with an interminable Cretan labyrinth of a figure, no two parts of which were of one precise shade--owing I suppose to his keeping his arm at sea unmethodically in sun and shade, his shirt sleeves irregularly rolled up at various times--this same arm of his, I say, looked for all the world like a strip of that same patchwork quilt. Indeed, partly lying on it as the arm did when I first awoke, I could hardly tell it from the quilt, they so blended their hues together; and it was only by the sense of weight and pressure that I could tell that Queequeg was hugging me.\n",
            "My sensations were strange. Let me try to explain them. When I was a child, I well remember a somewhat similar circumstance that befell me; whether it was a reality or a dream, I never could entirely settle. The circumstance was this. I had been cutting up some caper or other--I think it was trying to crawl up the chimney, as I had seen a little sweep do a few days previous; and my stepmother who, somehow or other, was all the time whipping me, or sending me to bed supperless,--my mother dragged me by the legs out of the chimney and packed me off to bed, though it was only two o'clock in the afternoon of the 21st June, the longest day in the year in our hemisphere. I felt dreadfully. But there was no help for it, so up stairs I went to my little room in the third floor, undressed myself as slowly as possible so as to kill time, and with a bitter sigh got between the sheets.\n",
            "I lay there dismally calculating that sixteen entire hours must elapse before I could hope for a resurrection. Sixteen hours in bed! the small of my back ached to think of it. And it was so light too; the sun shining in at the window, and a great rattling of coaches in the streets, and the sound of gay voices all over the house. I felt worse and worse--at last I got up, dressed, and softly going down in my stockinged feet, sought out my stepmother, and suddenly threw myself at her feet, beseeching her as a particular favour to give me a good slippering for my misbehaviour; anything indeed but condemning me to lie abed such an unendurable length of time. But she was the best and most conscientious of stepmothers, and back I had to go to my room. For several hours I lay there broad awake, feeling a great deal worse than I have ever done since, even from the greatest subsequent misfortunes. At last I must have fallen into a troubled nightmare of a doze; and slowly waking from it--half steeped in dreams--I opened my eyes, and the before sun-lit room was now wrapped in outer darkness. Instantly I felt a shock running through all my frame; nothing was to be seen, and nothing was to be heard; but a supernatural hand seemed placed in mine. My arm hung over the counterpane, and the nameless, unimaginable, silent form or phantom, to which the hand belonged, seemed closely seated by my bed-side. For what seemed ages piled on ages, I lay there, frozen with the most awful fears, not daring to drag away my hand; yet ever thinking that if I could but stir it one single inch, the horrid spell would be broken. I knew not how this consciousness at last glided away from me; but waking in the morning, I shudderingly remembered it all, and for days and weeks and months afterwards I lost myself in confounding attempts to explain the mystery. Nay, to this very hour, I often puzzle myself with it.\n",
            "Now, take away the awful fear, and my sensations at feeling the supernatural hand in mine were very similar, in their strangeness, to those which I experienced on waking up and seeing Queequeg's pagan arm thrown round me. But at length all the past night's events soberly recurred, one by one, in fixed reality, and then I lay only alive to the comical predicament. For though I tried to move his arm--unlock his bridegroom clasp--yet, sleeping as he was, he still hugged me tightly, as though naught but death should part us twain. I now strove to rouse him--\"Queequeg!\"--but his only answer was a snore. I then rolled over, my neck feeling as if it were in a horse-collar; and suddenly felt a slight scratch. Throwing aside the counterpane, there lay the tomahawk sleeping by the savage's side, as if it were a hatchet-faced baby. A pretty pickle, truly, thought I; abed here in a strange house in the broad day, with a cannibal and a tomahawk! \"Queequeg!--in the name of goodness, Queequeg, wake!\" At length, by dint of much wriggling, and loud and incessant expostulations upon the unbecomingness of his hugging a fellow male in that matrimonial sort of style, I succeeded in extracting a grunt; and presently, he drew back his arm, shook himself all over like a Newfoundland dog just from the water, and sat up in bed, stiff as a pike-staff, looking at me, and rubbing his eyes as if he did not altogether remember how I came to be there, though a dim consciousness of knowing something about me seemed slowly dawning over him. Meanwhile, I lay quietly eyeing him, having no serious misgivings now, and bent upon narrowly observing so curious a creature. When, at last, his mind seemed made up touching the character of his bedfellow, and he became, as it were, reconciled to the fact; he jumped out upon the floor, and by certain signs and sounds gave me to understand that, if it pleased me, he would dress first and then leave me to dress afterwards, leaving the whole apartment to myself. Thinks I, Queequeg, under the circumstances, this is a very civilized overture; but, the truth is, these savages have an innate sense of delicacy, say what you will; it is marvellous how essentially polite they are. I pay this particular compliment to Queequeg, because he treated me with so much civility and consideration, while I was guilty of great rudeness; staring at him from the bed, and watching all his toilette motions; for the time my curiosity getting the better of my breeding. Nevertheless, a man like Queequeg you don't see every day, he and his ways were well worth unusual regarding.\n",
            "He commenced dressing at top by donning his beaver hat, a very tall one, by the by, and then--still minus his trowsers--he hunted up his boots. What under the heavens he did it for, I cannot tell, but his next movement was to crush himself--boots in hand, and hat on--under the bed; when, from sundry violent gaspings and strainings, I inferred he was hard at work booting himself; though by no law of propriety that I ever heard of, is any man required to be private when putting on his boots. But Queequeg, do you see, was a creature in the transition stage--neither caterpillar nor butterfly. He was just enough civilized to show off his outlandishness in the strangest possible manners. His education was not yet completed. He was an undergraduate. If he had not been a small degree civilized, he very probably would not have troubled himself with boots at all; but then, if he had not been still a savage, he never would have dreamt of getting under the bed to put them on. At last, he emerged with his hat very much dented and crushed down over his eyes, and began creaking and limping about the room, as if, not being much accustomed to boots, his pair of damp, wrinkled cowhide ones--probably not made to order either--rather pinched and tormented him at the first go off of a bitter cold morning.\n",
            "Seeing, now, that there were no curtains to the window, and that the street being very narrow, the house opposite commanded a plain view into the room, and observing more and more the indecorous figure that Queequeg made, staving about with little else but his hat and boots on; I begged him as well as I could, to accelerate his toilet somewhat, and particularly to get into his pantaloons as soon as possible. He complied, and then proceeded to wash himself. At that time in the morning any Christian would have washed his face; but Queequeg, to my amazement, contented himself with restricting his ablutions to his chest, arms, and hands. He then donned his waistcoat, and taking up a piece of hard soap on the wash-stand centre table, dipped it into water and commenced lathering his face. I was watching to see where he kept his razor, when lo and behold, he takes the harpoon from the bed corner, slips out the long wooden stock, unsheathes the head, whets it a little on his boot, and striding up to the bit of mirror against the wall, begins a vigorous scraping, or rather harpooning of his cheeks. Thinks I, Queequeg, this is using Rogers's best cutlery with a vengeance. Afterwards I wondered the less at this operation when I came to know of what fine steel the head of a harpoon is made, and how exceedingly sharp the long straight edges are always kept.\n",
            "The rest of his toilet was soon achieved, and he proudly marched out of the room, wrapped up in his great pilot monkey jacket, and sporting his harpoon like a marshal's baton.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\",disable=['parser','tagger','ner'])"
      ],
      "metadata": {
        "id": "jnFPPfDFvALC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OqStXlK2gWK",
        "outputId": "4672e259-b960-4377-ce0b-7d128218a13b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_docx(file_path):\n",
        "    \"\"\"\n",
        "    Function to read text from a .docx file.\n",
        "\n",
        "    Returns:\n",
        "    str: Text extracted from the .docx file.\n",
        "    \"\"\"\n",
        "    doc =Document(file_path)\n",
        "    full_text= []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return '\\n'.join(full_text)\n",
        "data = read_docx(\"/content/moby_dick_ch4.docx\")"
      ],
      "metadata": {
        "id": "1MefuGFEN2Ht"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sep_pun(doc_text):\n",
        "  return [token.text.lower() for token in nlp.doc]"
      ],
      "metadata": {
        "id": "U0DFHx0p3dzY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def filter_special_characters(tokens):\n",
        "    \"\"\"\n",
        "    Function to filter out special characters and punctuation from a list of tokens.\n",
        "\n",
        "    Args:\n",
        "    tokens (list of str): List of tokens.\n",
        "\n",
        "    Returns:\n",
        "    list of str: List of tokens with special characters and punctuation removed.\n",
        "    \"\"\"\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        # Remove any character that is not a letter or a digit\n",
        "        cleaned_token = re.sub(r'[^a-zA-Z0-9]', '', token)\n",
        "        if cleaned_token:  # Only add non-empty tokens\n",
        "            filtered_tokens.append(cleaned_token)\n",
        "    return filtered_tokens\n",
        "\n",
        "# Example usage\n",
        "tokens = [\"hello,\", \"world!\", \"#NLP\", \"is\", \"fun.\", \"123\", \"@AI\"]\n",
        "filtered_tokens = filter_special_characters(tokens)\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZY5StzH4ESs",
        "outputId": "b51065eb-b69e-4426-ff11-5628a8f66212"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'NLP', 'is', 'fun', '123', 'AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_special_characters(tokens):\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        cleaned_token = re.sub(r'[^a-zA-Z0-9]', '', token.text)\n",
        "        if cleaned_token:  # Only add non-empty tokens\n",
        "            filtered_tokens.append(cleaned_token)\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "kpRgt4kIM7PM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=nlp(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqV1E77fZRKH",
        "outputId": "b7852896-8312-4ef5-af65-90fd19fcb38b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=filter_special_characters(text)"
      ],
      "metadata": {
        "id": "7XuxRG5MYuQE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(tokens)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0OuV-uhGS5L",
        "outputId": "6327bccb-ea82-4c43-f461-0c8c1f571b09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1670"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_length=25+1\n",
        "train_sequence=[]\n",
        "for i in range(train_length,len(tokens)):\n",
        "  seq=tokens[i-train_length:i]\n",
        "  train_sequence.append(seq)\n"
      ],
      "metadata": {
        "id": "b4EQnAnnZOhI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmjXJ1sAbU-H",
        "outputId": "c642b1fc-da30-412c-fba1-92c42fe7dc40"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequence[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SQuml2ybhBS",
        "outputId": "1127526e-d189-47b8-a5e8-81e4430126ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Upon',\n",
              " 'waking',\n",
              " 'next',\n",
              " 'morning',\n",
              " 'about',\n",
              " 'daylight',\n",
              " 'I',\n",
              " 'found',\n",
              " 'Queequeg',\n",
              " 's',\n",
              " 'arm',\n",
              " 'thrown',\n",
              " 'over',\n",
              " 'me',\n",
              " 'in',\n",
              " 'the',\n",
              " 'most',\n",
              " 'loving',\n",
              " 'and',\n",
              " 'affectionate',\n",
              " 'manner',\n",
              " 'You',\n",
              " 'had',\n",
              " 'almost',\n",
              " 'thought',\n",
              " 'I']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "345a-hZ7bi51"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()"
      ],
      "metadata": {
        "id": "ep1PDTFTcRBF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(train_sequence)"
      ],
      "metadata": {
        "id": "kOP7io6LcY7x"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=tokenizer.texts_to_sequences\n",
        "seq=sequences(train_sequence)"
      ],
      "metadata": {
        "id": "OEg3nkp8c-rk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.index_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHQGeyzodkng",
        "outputId": "ac8d8049-8b28-40e7-d1f8-1e51c83eb0eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'the',\n",
              " 2: 'and',\n",
              " 3: 'i',\n",
              " 4: 'a',\n",
              " 5: 'to',\n",
              " 6: 'his',\n",
              " 7: 'of',\n",
              " 8: 'in',\n",
              " 9: 'was',\n",
              " 10: 'he',\n",
              " 11: 'it',\n",
              " 12: 'at',\n",
              " 13: 'me',\n",
              " 14: 'as',\n",
              " 15: 'my',\n",
              " 16: 'but',\n",
              " 17: 'that',\n",
              " 18: 'up',\n",
              " 19: 'with',\n",
              " 20: 'by',\n",
              " 21: 'queequeg',\n",
              " 22: 'all',\n",
              " 23: 'for',\n",
              " 24: 'not',\n",
              " 25: 'this',\n",
              " 26: 'on',\n",
              " 27: 'bed',\n",
              " 28: 'arm',\n",
              " 29: 'were',\n",
              " 30: 'from',\n",
              " 31: 'if',\n",
              " 32: 'over',\n",
              " 33: 'when',\n",
              " 34: 'there',\n",
              " 35: 'very',\n",
              " 36: 'then',\n",
              " 37: 'had',\n",
              " 38: 'could',\n",
              " 39: 'so',\n",
              " 40: 'or',\n",
              " 41: 'lay',\n",
              " 42: 'have',\n",
              " 43: 'himself',\n",
              " 44: 'is',\n",
              " 45: 'boots',\n",
              " 46: 'room',\n",
              " 47: 'little',\n",
              " 48: 'no',\n",
              " 49: 'one',\n",
              " 50: 'time',\n",
              " 51: 'though',\n",
              " 52: 'myself',\n",
              " 53: 'last',\n",
              " 54: 'now',\n",
              " 55: 'be',\n",
              " 56: 'hand',\n",
              " 57: 'seemed',\n",
              " 58: 'would',\n",
              " 59: 'him',\n",
              " 60: 'out',\n",
              " 61: 's',\n",
              " 62: 'been',\n",
              " 63: 'an',\n",
              " 64: 'only',\n",
              " 65: 'felt',\n",
              " 66: 'into',\n",
              " 67: 'what',\n",
              " 68: 'how',\n",
              " 69: 'much',\n",
              " 70: 'made',\n",
              " 71: 'under',\n",
              " 72: 'hat',\n",
              " 73: 'you',\n",
              " 74: 'great',\n",
              " 75: 'about',\n",
              " 76: 'morning',\n",
              " 77: 'like',\n",
              " 78: 'waking',\n",
              " 79: 'upon',\n",
              " 80: 'counterpane',\n",
              " 81: 'which',\n",
              " 82: 'sun',\n",
              " 83: 'did',\n",
              " 84: 'first',\n",
              " 85: 'tell',\n",
              " 86: 'well',\n",
              " 87: 'do',\n",
              " 88: 'off',\n",
              " 89: 'day',\n",
              " 90: 'slowly',\n",
              " 91: 'possible',\n",
              " 92: 'hours',\n",
              " 93: 'back',\n",
              " 94: 'house',\n",
              " 95: 'worse',\n",
              " 96: 'length',\n",
              " 97: 'feeling',\n",
              " 98: 'ever',\n",
              " 99: 'eyes',\n",
              " 100: 'away',\n",
              " 101: 'yet',\n",
              " 102: 'afterwards',\n",
              " 103: 'still',\n",
              " 104: 'civilized',\n",
              " 105: 'see',\n",
              " 106: 'most',\n",
              " 107: 'harpoon',\n",
              " 108: 'patchwork',\n",
              " 109: 'figure',\n",
              " 110: 'two',\n",
              " 111: 'shade',\n",
              " 112: 'rolled',\n",
              " 113: 'same',\n",
              " 114: 'say',\n",
              " 115: 'quilt',\n",
              " 116: 'indeed',\n",
              " 117: 'they',\n",
              " 118: 'their',\n",
              " 119: 'sense',\n",
              " 120: 'hugging',\n",
              " 121: 'sensations',\n",
              " 122: 'strange',\n",
              " 123: 'explain',\n",
              " 124: 'them',\n",
              " 125: 'remember',\n",
              " 126: 'somewhat',\n",
              " 127: 'similar',\n",
              " 128: 'circumstance',\n",
              " 129: 'reality',\n",
              " 130: 'never',\n",
              " 131: 'other',\n",
              " 132: 'think',\n",
              " 133: 'chimney',\n",
              " 134: 'seen',\n",
              " 135: 'days',\n",
              " 136: 'stepmother',\n",
              " 137: 'floor',\n",
              " 138: 'bitter',\n",
              " 139: 'got',\n",
              " 140: 'sixteen',\n",
              " 141: 'must',\n",
              " 142: 'before',\n",
              " 143: 'small',\n",
              " 144: 'window',\n",
              " 145: 'down',\n",
              " 146: 'feet',\n",
              " 147: 'suddenly',\n",
              " 148: 'her',\n",
              " 149: 'particular',\n",
              " 150: 'abed',\n",
              " 151: 'best',\n",
              " 152: 'go',\n",
              " 153: 'broad',\n",
              " 154: 'troubled',\n",
              " 155: 'nothing',\n",
              " 156: 'heard',\n",
              " 157: 'supernatural',\n",
              " 158: 'mine',\n",
              " 159: 'side',\n",
              " 160: 'ages',\n",
              " 161: 'awful',\n",
              " 162: 'consciousness',\n",
              " 163: 'seeing',\n",
              " 164: 'sleeping',\n",
              " 165: 'tomahawk',\n",
              " 166: 'savage',\n",
              " 167: 'just',\n",
              " 168: 'water',\n",
              " 169: 'came',\n",
              " 170: 'observing',\n",
              " 171: 'creature',\n",
              " 172: 'dress',\n",
              " 173: 'thinks',\n",
              " 174: 'are',\n",
              " 175: 'watching',\n",
              " 176: 'getting',\n",
              " 177: 'man',\n",
              " 178: 'commenced',\n",
              " 179: 'hard',\n",
              " 180: 'any',\n",
              " 181: 'probably',\n",
              " 182: 'being',\n",
              " 183: 'rather',\n",
              " 184: 'more',\n",
              " 185: 'toilet',\n",
              " 186: 'soon',\n",
              " 187: 'wash',\n",
              " 188: 'face',\n",
              " 189: 'kept',\n",
              " 190: 'long',\n",
              " 191: 'head',\n",
              " 192: 'thought',\n",
              " 193: 'wrapped',\n",
              " 194: 'thrown',\n",
              " 195: 'next',\n",
              " 196: 'wife',\n",
              " 197: 'full',\n",
              " 198: 'odd',\n",
              " 199: 'parti',\n",
              " 200: 'coloured',\n",
              " 201: 'squares',\n",
              " 202: 'triangles',\n",
              " 203: 'tattooed',\n",
              " 204: 'interminable',\n",
              " 205: 'cretan',\n",
              " 206: 'labyrinth',\n",
              " 207: 'parts',\n",
              " 208: 'precise',\n",
              " 209: 'owing',\n",
              " 210: 'suppose',\n",
              " 211: 'keeping',\n",
              " 212: 'sea',\n",
              " 213: 'unmethodically',\n",
              " 214: 'shirt',\n",
              " 215: 'sleeves',\n",
              " 216: 'irregularly',\n",
              " 217: 'various',\n",
              " 218: 'times',\n",
              " 219: 'looked',\n",
              " 220: 'world',\n",
              " 221: 'strip',\n",
              " 222: 'partly',\n",
              " 223: 'lying',\n",
              " 224: 'awoke',\n",
              " 225: 'hardly',\n",
              " 226: 'blended',\n",
              " 227: 'hues',\n",
              " 228: 'together',\n",
              " 229: 'weight',\n",
              " 230: 'pressure',\n",
              " 231: 'let',\n",
              " 232: 'try',\n",
              " 233: 'child',\n",
              " 234: 'befell',\n",
              " 235: 'whether',\n",
              " 236: 'dream',\n",
              " 237: 'entirely',\n",
              " 238: 'settle',\n",
              " 239: 'cutting',\n",
              " 240: 'some',\n",
              " 241: 'caper',\n",
              " 242: 'trying',\n",
              " 243: 'crawl',\n",
              " 244: 'sweep',\n",
              " 245: 'few',\n",
              " 246: 'previous',\n",
              " 247: 'who',\n",
              " 248: 'somehow',\n",
              " 249: 'whipping',\n",
              " 250: 'sending',\n",
              " 251: 'supperlessmy',\n",
              " 252: 'mother',\n",
              " 253: 'dragged',\n",
              " 254: 'legs',\n",
              " 255: 'packed',\n",
              " 256: 'oclock',\n",
              " 257: 'afternoon',\n",
              " 258: '21st',\n",
              " 259: 'june',\n",
              " 260: 'longest',\n",
              " 261: 'year',\n",
              " 262: 'our',\n",
              " 263: 'hemisphere',\n",
              " 264: 'dreadfully',\n",
              " 265: 'help',\n",
              " 266: 'stairs',\n",
              " 267: 'went',\n",
              " 268: 'third',\n",
              " 269: 'undressed',\n",
              " 270: 'kill',\n",
              " 271: 'sigh',\n",
              " 272: 'between',\n",
              " 273: 'sheets',\n",
              " 274: 'dismally',\n",
              " 275: 'calculating',\n",
              " 276: 'entire',\n",
              " 277: 'elapse',\n",
              " 278: 'hope',\n",
              " 279: 'resurrection',\n",
              " 280: 'ached',\n",
              " 281: 'light',\n",
              " 282: 'too',\n",
              " 283: 'shining',\n",
              " 284: 'rattling',\n",
              " 285: 'coaches',\n",
              " 286: 'streets',\n",
              " 287: 'sound',\n",
              " 288: 'gay',\n",
              " 289: 'voices',\n",
              " 290: 'dressed',\n",
              " 291: 'softly',\n",
              " 292: 'going',\n",
              " 293: 'stockinged',\n",
              " 294: 'sought',\n",
              " 295: 'threw',\n",
              " 296: 'beseeching',\n",
              " 297: 'favour',\n",
              " 298: 'give',\n",
              " 299: 'good',\n",
              " 300: 'slippering',\n",
              " 301: 'misbehaviour',\n",
              " 302: 'anything',\n",
              " 303: 'condemning',\n",
              " 304: 'lie',\n",
              " 305: 'such',\n",
              " 306: 'unendurable',\n",
              " 307: 'she',\n",
              " 308: 'conscientious',\n",
              " 309: 'stepmothers',\n",
              " 310: 'several',\n",
              " 311: 'awake',\n",
              " 312: 'deal',\n",
              " 313: 'than',\n",
              " 314: 'done',\n",
              " 315: 'since',\n",
              " 316: 'even',\n",
              " 317: 'greatest',\n",
              " 318: 'subsequent',\n",
              " 319: 'misfortunes',\n",
              " 320: 'fallen',\n",
              " 321: 'nightmare',\n",
              " 322: 'doze',\n",
              " 323: 'half',\n",
              " 324: 'steeped',\n",
              " 325: 'dreams',\n",
              " 326: 'opened',\n",
              " 327: 'lit',\n",
              " 328: 'outer',\n",
              " 329: 'darkness',\n",
              " 330: 'instantly',\n",
              " 331: 'shock',\n",
              " 332: 'running',\n",
              " 333: 'through',\n",
              " 334: 'frame',\n",
              " 335: 'placed',\n",
              " 336: 'hung',\n",
              " 337: 'nameless',\n",
              " 338: 'unimaginable',\n",
              " 339: 'silent',\n",
              " 340: 'form',\n",
              " 341: 'phantom',\n",
              " 342: 'belonged',\n",
              " 343: 'closely',\n",
              " 344: 'seated',\n",
              " 345: 'piled',\n",
              " 346: 'frozen',\n",
              " 347: 'fears',\n",
              " 348: 'daring',\n",
              " 349: 'drag',\n",
              " 350: 'thinking',\n",
              " 351: 'stir',\n",
              " 352: 'single',\n",
              " 353: 'inch',\n",
              " 354: 'horrid',\n",
              " 355: 'spell',\n",
              " 356: 'broken',\n",
              " 357: 'knew',\n",
              " 358: 'glided',\n",
              " 359: 'shudderingly',\n",
              " 360: 'remembered',\n",
              " 361: 'weeks',\n",
              " 362: 'months',\n",
              " 363: 'lost',\n",
              " 364: 'confounding',\n",
              " 365: 'attempts',\n",
              " 366: 'mystery',\n",
              " 367: 'nay',\n",
              " 368: 'hour',\n",
              " 369: 'often',\n",
              " 370: 'puzzle',\n",
              " 371: 'take',\n",
              " 372: 'fear',\n",
              " 373: 'strangeness',\n",
              " 374: 'those',\n",
              " 375: 'experienced',\n",
              " 376: 'pagan',\n",
              " 377: 'round',\n",
              " 378: 'past',\n",
              " 379: 'night',\n",
              " 380: 'events',\n",
              " 381: 'soberly',\n",
              " 382: 'recurred',\n",
              " 383: 'fixed',\n",
              " 384: 'alive',\n",
              " 385: 'comical',\n",
              " 386: 'predicament',\n",
              " 387: 'tried',\n",
              " 388: 'move',\n",
              " 389: 'unlock',\n",
              " 390: 'bridegroom',\n",
              " 391: 'clasp',\n",
              " 392: 'hugged',\n",
              " 393: 'tightly',\n",
              " 394: 'naught',\n",
              " 395: 'death',\n",
              " 396: 'should',\n",
              " 397: 'part',\n",
              " 398: 'us',\n",
              " 399: 'twain',\n",
              " 400: 'strove',\n",
              " 401: 'rouse',\n",
              " 402: 'himqueequegbut',\n",
              " 403: 'answer',\n",
              " 404: 'snore',\n",
              " 405: 'neck',\n",
              " 406: 'horse',\n",
              " 407: 'collar',\n",
              " 408: 'slight',\n",
              " 409: 'scratch',\n",
              " 410: 'throwing',\n",
              " 411: 'aside',\n",
              " 412: 'hatchet',\n",
              " 413: 'faced',\n",
              " 414: 'baby',\n",
              " 415: 'pretty',\n",
              " 416: 'pickle',\n",
              " 417: 'truly',\n",
              " 418: 'here',\n",
              " 419: 'cannibal',\n",
              " 420: 'queequegin',\n",
              " 421: 'name',\n",
              " 422: 'goodness',\n",
              " 423: 'wake',\n",
              " 424: 'dint',\n",
              " 425: 'wriggling',\n",
              " 426: 'loud',\n",
              " 427: 'incessant',\n",
              " 428: 'expostulations',\n",
              " 429: 'unbecomingness',\n",
              " 430: 'fellow',\n",
              " 431: 'male',\n",
              " 432: 'matrimonial',\n",
              " 433: 'sort',\n",
              " 434: 'style',\n",
              " 435: 'succeeded',\n",
              " 436: 'extracting',\n",
              " 437: 'grunt',\n",
              " 438: 'presently',\n",
              " 439: 'drew',\n",
              " 440: 'shook',\n",
              " 441: 'newfoundland',\n",
              " 442: 'dog',\n",
              " 443: 'sat',\n",
              " 444: 'stiff',\n",
              " 445: 'pike',\n",
              " 446: 'staff',\n",
              " 447: 'looking',\n",
              " 448: 'rubbing',\n",
              " 449: 'altogether',\n",
              " 450: 'dim',\n",
              " 451: 'knowing',\n",
              " 452: 'something',\n",
              " 453: 'dawning',\n",
              " 454: 'meanwhile',\n",
              " 455: 'quietly',\n",
              " 456: 'eyeing',\n",
              " 457: 'having',\n",
              " 458: 'serious',\n",
              " 459: 'misgivings',\n",
              " 460: 'bent',\n",
              " 461: 'narrowly',\n",
              " 462: 'curious',\n",
              " 463: 'mind',\n",
              " 464: 'touching',\n",
              " 465: 'character',\n",
              " 466: 'bedfellow',\n",
              " 467: 'became',\n",
              " 468: 'reconciled',\n",
              " 469: 'fact',\n",
              " 470: 'jumped',\n",
              " 471: 'certain',\n",
              " 472: 'signs',\n",
              " 473: 'sounds',\n",
              " 474: 'gave',\n",
              " 475: 'understand',\n",
              " 476: 'pleased',\n",
              " 477: 'leave',\n",
              " 478: 'leaving',\n",
              " 479: 'whole',\n",
              " 480: 'apartment',\n",
              " 481: 'circumstances',\n",
              " 482: 'overture',\n",
              " 483: 'truth',\n",
              " 484: 'these',\n",
              " 485: 'savages',\n",
              " 486: 'innate',\n",
              " 487: 'delicacy',\n",
              " 488: 'will',\n",
              " 489: 'marvellous',\n",
              " 490: 'essentially',\n",
              " 491: 'polite',\n",
              " 492: 'pay',\n",
              " 493: 'compliment',\n",
              " 494: 'because',\n",
              " 495: 'treated',\n",
              " 496: 'civility',\n",
              " 497: 'consideration',\n",
              " 498: 'while',\n",
              " 499: 'guilty',\n",
              " 500: 'rudeness',\n",
              " 501: 'staring',\n",
              " 502: 'toilette',\n",
              " 503: 'motions',\n",
              " 504: 'curiosity',\n",
              " 505: 'better',\n",
              " 506: 'breeding',\n",
              " 507: 'nevertheless',\n",
              " 508: 'nt',\n",
              " 509: 'every',\n",
              " 510: 'ways',\n",
              " 511: 'worth',\n",
              " 512: 'unusual',\n",
              " 513: 'regarding',\n",
              " 514: 'dressing',\n",
              " 515: 'top',\n",
              " 516: 'donning',\n",
              " 517: 'beaver',\n",
              " 518: 'tall',\n",
              " 519: 'minus',\n",
              " 520: 'trowsers',\n",
              " 521: 'hunted',\n",
              " 522: 'heavens',\n",
              " 523: 'can',\n",
              " 524: 'movement',\n",
              " 525: 'crush',\n",
              " 526: 'sundry',\n",
              " 527: 'violent',\n",
              " 528: 'gaspings',\n",
              " 529: 'strainings',\n",
              " 530: 'inferred',\n",
              " 531: 'work',\n",
              " 532: 'booting',\n",
              " 533: 'law',\n",
              " 534: 'propriety',\n",
              " 535: 'required',\n",
              " 536: 'private',\n",
              " 537: 'putting',\n",
              " 538: 'transition',\n",
              " 539: 'stage',\n",
              " 540: 'neither',\n",
              " 541: 'caterpillar',\n",
              " 542: 'nor',\n",
              " 543: 'butterfly',\n",
              " 544: 'enough',\n",
              " 545: 'show',\n",
              " 546: 'outlandishness',\n",
              " 547: 'strangest',\n",
              " 548: 'manners',\n",
              " 549: 'education',\n",
              " 550: 'completed',\n",
              " 551: 'undergraduate',\n",
              " 552: 'degree',\n",
              " 553: 'dreamt',\n",
              " 554: 'put',\n",
              " 555: 'emerged',\n",
              " 556: 'dented',\n",
              " 557: 'crushed',\n",
              " 558: 'began',\n",
              " 559: 'creaking',\n",
              " 560: 'limping',\n",
              " 561: 'accustomed',\n",
              " 562: 'pair',\n",
              " 563: 'damp',\n",
              " 564: 'wrinkled',\n",
              " 565: 'cowhide',\n",
              " 566: 'ones',\n",
              " 567: 'order',\n",
              " 568: 'either',\n",
              " 569: 'pinched',\n",
              " 570: 'tormented',\n",
              " 571: 'cold',\n",
              " 572: 'curtains',\n",
              " 573: 'street',\n",
              " 574: 'narrow',\n",
              " 575: 'opposite',\n",
              " 576: 'commanded',\n",
              " 577: 'plain',\n",
              " 578: 'view',\n",
              " 579: 'indecorous',\n",
              " 580: 'staving',\n",
              " 581: 'else',\n",
              " 582: 'begged',\n",
              " 583: 'accelerate',\n",
              " 584: 'particularly',\n",
              " 585: 'get',\n",
              " 586: 'pantaloons',\n",
              " 587: 'complied',\n",
              " 588: 'proceeded',\n",
              " 589: 'christian',\n",
              " 590: 'washed',\n",
              " 591: 'amazement',\n",
              " 592: 'contented',\n",
              " 593: 'restricting',\n",
              " 594: 'ablutions',\n",
              " 595: 'chest',\n",
              " 596: 'arms',\n",
              " 597: 'hands',\n",
              " 598: 'donned',\n",
              " 599: 'waistcoat',\n",
              " 600: 'taking',\n",
              " 601: 'piece',\n",
              " 602: 'soap',\n",
              " 603: 'stand',\n",
              " 604: 'centre',\n",
              " 605: 'table',\n",
              " 606: 'dipped',\n",
              " 607: 'lathering',\n",
              " 608: 'where',\n",
              " 609: 'razor',\n",
              " 610: 'lo',\n",
              " 611: 'behold',\n",
              " 612: 'takes',\n",
              " 613: 'corner',\n",
              " 614: 'slips',\n",
              " 615: 'wooden',\n",
              " 616: 'stock',\n",
              " 617: 'unsheathes',\n",
              " 618: 'whets',\n",
              " 619: 'boot',\n",
              " 620: 'striding',\n",
              " 621: 'bit',\n",
              " 622: 'mirror',\n",
              " 623: 'against',\n",
              " 624: 'wall',\n",
              " 625: 'begins',\n",
              " 626: 'vigorous',\n",
              " 627: 'scraping',\n",
              " 628: 'harpooning',\n",
              " 629: 'cheeks',\n",
              " 630: 'using',\n",
              " 631: 'rogers',\n",
              " 632: 'cutlery',\n",
              " 633: 'vengeance',\n",
              " 634: 'wondered',\n",
              " 635: 'less',\n",
              " 636: 'operation',\n",
              " 637: 'know',\n",
              " 638: 'fine',\n",
              " 639: 'steel',\n",
              " 640: 'exceedingly',\n",
              " 641: 'sharp',\n",
              " 642: 'straight',\n",
              " 643: 'edges',\n",
              " 644: 'always',\n",
              " 645: 'rest',\n",
              " 646: 'achieved',\n",
              " 647: 'almost',\n",
              " 648: 'proudly',\n",
              " 649: 'manner',\n",
              " 650: 'marched',\n",
              " 651: 'affectionate',\n",
              " 652: 'loving',\n",
              " 653: 'pilot',\n",
              " 654: 'monkey',\n",
              " 655: 'jacket',\n",
              " 656: 'found',\n",
              " 657: 'sporting',\n",
              " 658: 'daylight',\n",
              " 659: 'marshal'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE6sdqrRfdVX",
        "outputId": "26e867ce-6e11-4a15-9fef-ceb63e05c47d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "659"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "hjS8ZSntgE4u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "j2ri0Kwbg4Rw",
        "outputId": "483cfa07-2b19-4108-beb0-baeb3b2ef580"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Tokenizer.texts_to_sequences of <keras.src.legacy.preprocessing.text.Tokenizer object at 0x7c61dc34fac0>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>keras.src.legacy.preprocessing.text.Tokenizer.texts_to_sequences</b><br/>def texts_to_sequences(texts)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/text.py</a>&lt;no docstring&gt;</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 176);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq=np.array(seq)"
      ],
      "metadata": {
        "id": "H6tTVxdVgdf9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CkOxqzbgs0N",
        "outputId": "f219664a-cb83-4079-cddb-8000eeb4cf5e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 79  78 195 ... 647 192   3]\n",
            " [ 78 195  76 ... 192   3  37]\n",
            " [195  76  75 ...   3  37  62]\n",
            " ...\n",
            " [185   9 186 ... 107  77   4]\n",
            " [  9 186 646 ...  77   4 659]\n",
            " [186 646   2 ...   4 659  61]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = seq[:,:-1]\n",
        "Y = seq[:,-1]"
      ],
      "metadata": {
        "id": "n9t-poWLgyoF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=X.shape"
      ],
      "metadata": {
        "id": "R29FG2HuA-Ic"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "PzSvrCD2JaxF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=to_categorical(Y)"
      ],
      "metadata": {
        "id": "U_gJTykz_VZp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "rnbl0Qw8CY0p"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "TD0brxDS_9oc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.layers import Dense,LSTM,Embedding"
      ],
      "metadata": {
        "id": "L8nVOtivCW6I"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy"
      ],
      "metadata": {
        "id": "bar-sQZIJjWX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbbfqlnMNl9D",
        "outputId": "190251d6-8fad-40bc-d7c3-3b93ed99a5af"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1644"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model(vocabulary_size, sequence_length, embedding_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length))\n",
        "    model.add(LSTM(50, return_sequences=True))\n",
        "    model.add(LSTM(50))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "    model.compile(loss=CategoricalCrossentropy(), optimizer=Adam(), metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WaZ6X75nCmRe"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zci7v5ZiIaUM",
        "outputId": "55bfc514-3a7a-4585-9050-fa56a157fcab"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1644, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZaHNXStGs5a",
        "outputId": "a7fd6887-5857-4d23-da9d-927aaa8f2954"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1644, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length=X.shape[1]"
      ],
      "metadata": {
        "id": "7We1Hm1nGylA"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aubeEW1LUcZ",
        "outputId": "d60871e0-3c5c-46b0-e9e0-455f52698965"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1644, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoTPgkoTLaxY",
        "outputId": "5fc8b5cc-8d2b-437b-a737-c61f1eec1a46"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1644, 660)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A71VQVnoLKv4",
        "outputId": "08bde621-37c5-4bb4-860a-504137c8a4fe"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "659"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size=len(tokenizer.word_counts)"
      ],
      "metadata": {
        "id": "3Iwu9bEpGq8K"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_model(vocabulary_size+1,sequence_length,50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "0LxgPRgDF9Qu",
        "outputId": "37c82c9f-fe30-46df-f1e6-81a0dbfb251d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import dump,load"
      ],
      "metadata": {
        "id": "iVXOe4_uHDrT"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y,batch_size=128,epochs=50,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYFL_9_eIyO0",
        "outputId": "8c635294-6625-4eb4-c3c7-9f8cc7a6d288"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9179 - loss: 0.3746\n",
            "Epoch 2/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9089 - loss: 0.3661\n",
            "Epoch 3/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9149 - loss: 0.3776\n",
            "Epoch 4/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9165 - loss: 0.3535\n",
            "Epoch 5/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9289 - loss: 0.3162\n",
            "Epoch 6/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9370 - loss: 0.3096\n",
            "Epoch 7/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9259 - loss: 0.3315\n",
            "Epoch 8/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.9248 - loss: 0.3249\n",
            "Epoch 9/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.9233 - loss: 0.3189\n",
            "Epoch 10/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9449 - loss: 0.2759\n",
            "Epoch 11/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9452 - loss: 0.2534\n",
            "Epoch 12/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9492 - loss: 0.2492\n",
            "Epoch 13/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9485 - loss: 0.2517\n",
            "Epoch 14/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9416 - loss: 0.2471\n",
            "Epoch 15/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9537 - loss: 0.2401\n",
            "Epoch 16/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9530 - loss: 0.2308\n",
            "Epoch 17/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9393 - loss: 0.2789\n",
            "Epoch 18/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9295 - loss: 0.3017\n",
            "Epoch 19/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9479 - loss: 0.2760\n",
            "Epoch 20/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9515 - loss: 0.2613\n",
            "Epoch 21/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9404 - loss: 0.2959\n",
            "Epoch 22/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9491 - loss: 0.2370\n",
            "Epoch 23/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9475 - loss: 0.2579\n",
            "Epoch 24/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.9615 - loss: 0.2242\n",
            "Epoch 25/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9629 - loss: 0.2056\n",
            "Epoch 26/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9611 - loss: 0.2020\n",
            "Epoch 27/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9632 - loss: 0.1799\n",
            "Epoch 28/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9698 - loss: 0.1611\n",
            "Epoch 29/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9697 - loss: 0.1613\n",
            "Epoch 30/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9750 - loss: 0.1618\n",
            "Epoch 31/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9759 - loss: 0.1560\n",
            "Epoch 32/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9693 - loss: 0.1569\n",
            "Epoch 33/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9710 - loss: 0.1571\n",
            "Epoch 34/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9710 - loss: 0.1658\n",
            "Epoch 35/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9719 - loss: 0.1559\n",
            "Epoch 36/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9717 - loss: 0.1637\n",
            "Epoch 37/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.9663 - loss: 0.1662\n",
            "Epoch 38/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.9686 - loss: 0.1555\n",
            "Epoch 39/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.9761 - loss: 0.1412\n",
            "Epoch 40/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9700 - loss: 0.1597\n",
            "Epoch 41/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9728 - loss: 0.1551\n",
            "Epoch 42/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9741 - loss: 0.1525\n",
            "Epoch 43/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9697 - loss: 0.1533\n",
            "Epoch 44/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9750 - loss: 0.1357\n",
            "Epoch 45/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9730 - loss: 0.1536\n",
            "Epoch 46/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9759 - loss: 0.1363\n",
            "Epoch 47/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9756 - loss: 0.1459\n",
            "Epoch 48/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9790 - loss: 0.1347\n",
            "Epoch 49/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9803 - loss: 0.1298\n",
            "Epoch 50/50\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.9772 - loss: 0.1300\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c61ca4ea080>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_weights('moby_dick_ch4.weights.h5')\n"
      ],
      "metadata": {
        "id": "3oaaYV-3JfFK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save('RNN_T_Moby_dick.h5')\n"
      ],
      "metadata": {
        "id": "6DAVNlJqQfrE",
        "outputId": "bddd87fe-67a3-40b9-811a-dc623a1e4820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    return tokenizer.index_word[predicted[0]]\n",
        "\n",
        "seed_text = \"Once upon a time\"\n",
        "next_word = predict_next_word(model, tokenizer, seed_text, max_sequence_len)\n",
        "print(f\"Next word prediction: {next_word}\")"
      ],
      "metadata": {
        "id": "Qi2ig-5J02ad",
        "outputId": "9c916644-6abb-4b12-c972-111dcbcdd2bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Next word prediction: were\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFLE5-6H2UWD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}